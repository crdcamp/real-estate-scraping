{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3af6db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7237cda5",
   "metadata": {},
   "source": [
    "# Getting Most Recent Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416fdaee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading database PPIs for comparison...\n"
     ]
    }
   ],
   "source": [
    "def get_database_ppis():\n",
    "    \"\"\"Retrieve all PPIs from the database (layer 12) for comparison\"\"\"\n",
    "    url = \"https://gis.summitcountyco.gov/arcgis/rest/services/ParcelQueryTool/SummitMap1_Pro321/MapServer/12/query\"\n",
    "    \n",
    "    all_features = []\n",
    "    batch_size = 1000\n",
    "    offset = 0\n",
    "    params = {\n",
    "        \"where\": \"1=1\",\n",
    "        \"outFields\": \"*\",  # Get all fields for matching features\n",
    "        \"f\": \"json\",\n",
    "        \"returnGeometry\": \"false\",\n",
    "        \"outSR\": \"102654\",\n",
    "        \"resultRecordCount\": batch_size\n",
    "    }\n",
    "    \n",
    "    print(\"Loading database PPIs for comparison...\")\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            params[\"resultOffset\"] = offset\n",
    "            \n",
    "            response = requests.get(url, params=params)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            data = response.json()\n",
    "            \n",
    "            if \"features\" not in data or not data[\"features\"]:\n",
    "                break\n",
    "                \n",
    "            all_features.extend(data[\"features\"])\n",
    "            \n",
    "            if len(data[\"features\"]) < batch_size:\n",
    "                break\n",
    "                \n",
    "            offset += batch_size\n",
    "        \n",
    "        # Create a dictionary mapping PPI to feature data\n",
    "        ppi_to_feature = {}\n",
    "        for feature in all_features:\n",
    "            ppi = feature.get(\"attributes\", {}).get(\"PPI\")\n",
    "            if ppi is not None:\n",
    "                ppi_to_feature[ppi] = feature\n",
    "        \n",
    "        print(f\"Loaded {len(ppi_to_feature)} unique PPIs from database\")\n",
    "        return ppi_to_feature\n",
    "        \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error querying the database: {e}\")\n",
    "        return {}\n",
    "\n",
    "def main():\n",
    "    num_results = 5000\n",
    "    \n",
    "    # Get database PPIs for comparison\n",
    "    database_ppis = get_database_ppis()\n",
    "    \n",
    "    if not database_ppis:\n",
    "        print(\"Failed to load database PPIs. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Query URL with adjustable result count\n",
    "    query_url = f\"https://gis.summitcountyco.gov/arcgis/rest/services/ParcelQueryTool/SummitMap1_Pro321/MapServer/19/query?where=SOURCE=1&orderByFields=MODDATE%20DESC&resultRecordCount={num_results}&outFields=*&returnGeometry=false&f=json\"\n",
    "    \n",
    "    # Send the request\n",
    "    response = requests.get(query_url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        features = data.get(\"features\", [])\n",
    "        \n",
    "        matching_count = 0\n",
    "        results_json = []  # Store results as raw JSON\n",
    "        \n",
    "        # Define field lists at the beginning of the loop\n",
    "        modtype_fields = [\n",
    "            'AREA', 'PERIMETER', 'OBJECTID', 'PPI', 'SOURCE', \n",
    "            'MODDATE', 'MODTYPE'\n",
    "        ]\n",
    "        \n",
    "        schedule_fields = [\n",
    "            'OBJECTID_1', 'OBJECTID', 'PPI', 'Schedule', 'EcoCode', 'EcoDesc',\n",
    "            'NhoodCode', 'NhoodDescr', 'SubCode', 'SubName', 'SecondID', 'ShortDesc',\n",
    "            'AddressID', 'StreetID', 'SitusAdd', 'HouseNum', 'FullStreet', 'StreetName',\n",
    "            'TownCode', 'TownName', 'OwnerAdd1', 'OwnerAdd2', 'OwnerCity', 'OwnerState',\n",
    "            'PostCode', 'FullAdd', 'TotAcres', 'TotSqFt', 'YearBuilt', 'ExtWallMat',\n",
    "            'ExtWallHgt', 'HeatType', 'SquareFeet', 'SqeFtLiving', 'Unfinished',\n",
    "            'BsmtType', 'GarageType', 'NumOfCars', 'GarSqFt', 'NumOfRms', 'NumBedRms',\n",
    "            'NumLofts', 'NumKitch', 'MasterBath', 'FullBath', 'TqtrBaths', 'HalfBaths',\n",
    "            'QtrBaths', 'TotBath', 'MobHtitle', 'FloorLevel', 'ImpPos'\n",
    "        ]\n",
    "        \n",
    "        for i, feature in enumerate(features, 1):\n",
    "            attributes = feature.get(\"attributes\", {})\n",
    "            ppi = attributes.get(\"PPI\")\n",
    "            mod_date = attributes.get(\"MODDATE\")\n",
    "            \n",
    "            # Convert MODDATE (Unix timestamp in milliseconds) to readable date\n",
    "            if mod_date:\n",
    "                mod_date_readable = datetime.fromtimestamp(mod_date / 1000).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            else:\n",
    "                mod_date_readable = \"N/A\"\n",
    "            \n",
    "            # Only print if PPI exists in database\n",
    "            if ppi in database_ppis:\n",
    "                matching_count += 1\n",
    "                database_feature = database_ppis[ppi]\n",
    "                database_attributes = database_feature.get(\"attributes\", {})\n",
    "                \n",
    "                if matching_count == 1:  # Print header only when first match is found\n",
    "                    print(f\"\\nMatching PPIs from Most Recently Modified Parcels:\")\n",
    "                \n",
    "                print(f\"\\n--- Match #{matching_count} - PPI: {ppi} ---\")\n",
    "                print(f\"Modified Date: {mod_date_readable}\")\n",
    "                \n",
    "                # Collect match data for JSON storage\n",
    "                match_data = {\n",
    "                    'PPI': ppi,\n",
    "                    'Modified Date': mod_date_readable,\n",
    "                    'Raw Modified Date': mod_date,\n",
    "                    'MODTYPE Attributes': {},\n",
    "                    'Schedule ID Attributes': {}\n",
    "                }\n",
    "                \n",
    "                # Collect MODTYPE attributes\n",
    "                for field in modtype_fields:\n",
    "                    if field in attributes:\n",
    "                        match_data['MODTYPE Attributes'][field] = attributes[field]\n",
    "                \n",
    "                # Collect Schedule ID attributes\n",
    "                for field in schedule_fields:\n",
    "                    if field in database_attributes:\n",
    "                        match_data['Schedule ID Attributes'][field] = database_attributes[field]\n",
    "                \n",
    "                # Add to results JSON\n",
    "                results_json.append(match_data)\n",
    "                \n",
    "                # Print MODTYPE attributes (from layer 19)\n",
    "                print(\"\\nMODTYPE ATTRIBUTES:\")\n",
    "                for field in modtype_fields:\n",
    "                    if field in attributes:\n",
    "                        value = attributes[field]\n",
    "                        if field == 'MODDATE' and value:\n",
    "                            # Convert timestamp for MODDATE display\n",
    "                            value = datetime.fromtimestamp(value / 1000).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                        print(f\"  {field}: {value}\")\n",
    "                \n",
    "                # Print Schedule ID attributes (from layer 12)\n",
    "                print(\"\\nSCHEDULE ID ATTRIBUTES:\")\n",
    "                for field in schedule_fields:\n",
    "                    if field in database_attributes:\n",
    "                        value = database_attributes[field]\n",
    "                        print(f\"  {field}: {value}\")\n",
    "                \n",
    "                print(\"-\" * 50)\n",
    "        \n",
    "        # Summary\n",
    "        if matching_count > 0:\n",
    "            print(f\"\\nSUMMARY:\")\n",
    "            print(f\"Total queried parcels: {len(features)}\")\n",
    "            print(f\"Matching PPIs found in database: {matching_count}\")\n",
    "            print(f\"Match percentage: {(matching_count/len(features)*100):.1f}%\")\n",
    "            \n",
    "            # Return the JSON results for further processing\n",
    "            return results_json\n",
    "        else:\n",
    "            print(f\"\\nNo matching PPIs found in database out of {len(features)} queried parcels.\")\n",
    "            return []\n",
    "        \n",
    "    else:\n",
    "        print(f\"Error: Unable to fetch data. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "json_results = main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f76972f",
   "metadata": {},
   "source": [
    "# Scraping Schedul ID URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf130511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORD\n"
     ]
    }
   ],
   "source": [
    "def scrape_json_results(results):\n",
    "    for result in results:\n",
    "        schedule_value = result['Schedule ID Attributes']['Schedule']\n",
    "        schedule_url = f'https://gis.summitcountyco.gov/map/DetailData.aspx?Schno={schedule_value}'\n",
    "\n",
    "        if schedule_url:\n",
    "            r = requests.get(schedule_url)\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "            # Find the Document Type header, then get the data row below it\n",
    "            doc_header = soup.select_one('td.docLink')\n",
    "            \n",
    "            # Add error handling in case docLink doesn't exist\n",
    "            if doc_header:\n",
    "                data_row = doc_header.parent.find_next_sibling('tr')\n",
    "                doc_type_cell = data_row.select('td.style2b')[2]\n",
    "                doc_type_text = doc_type_cell.text.strip()  # Get the text and strip whitespace\n",
    "                \n",
    "                # Fix the conditional logic\n",
    "                if doc_type_text == 'B&S' or doc_type_text == 'BSA':\n",
    "                    print(schedule_url)\n",
    "            else:\n",
    "                print(f\"No docLink found for {schedule_url}\")\n",
    "\n",
    "scrape_json_results(json_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
